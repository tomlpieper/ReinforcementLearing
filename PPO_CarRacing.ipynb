{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n",
      "GPUs [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Track generation: 1078..1355 -> 277-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1239..1553 -> 314-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/Caskroom/miniforge/base/envs/ann/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "2022-07-10 14:04:47.442048: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-10 14:04:47.442159: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-07-10 14:04:47.447654: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-07-10 14:04:47.447733: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-10 14:04:47.455453: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-10 14:04:47.491935: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 14:04:49.770279: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-10 14:04:49.942844: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-10 14:04:49.952198: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-10 14:04:49.962664: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "#================================================================\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1:cpu, 0:first gpu\n",
    "import random\n",
    "import datetime\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# tensorboard init\n",
    "from tensorboardX import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "log_folder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development\n",
    "tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad, Adadelta\n",
    "from tensorflow.keras import backend as K\n",
    "import copy\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "from threading import Thread, Lock\n",
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    print(f'GPUs {gpus}')\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError: pass\n",
    "\n",
    "class Environment(Process):\n",
    "    def __init__(self, env_idx, child_conn, env_name, state_size, action_size, visualize=False):\n",
    "        super(Environment, self).__init__()\n",
    "        self.env = gym.make(env_name)\n",
    "        self.is_render = visualize\n",
    "        self.env_idx = env_idx\n",
    "        self.child_conn = child_conn\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def run(self):\n",
    "        super(Environment, self).run()\n",
    "        state = self.env.reset()\n",
    "        #state = np.reshape(state, [1, self.state_size])\n",
    "        self.child_conn.send(state)\n",
    "        while True:\n",
    "            action = self.child_conn.recv()\n",
    "            #if self.is_render and self.env_idx == 0:\n",
    "                #self.env.render()\n",
    "\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            self.child_conn.send([state, reward, done, info])\n",
    "\n",
    "\n",
    "class Actor_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        X = Conv2D(filters=6, kernel_size=(7,7), strides=3, activation='relu')(X_input)\n",
    "        X = MaxPooling2D()(X)\n",
    "        X = Conv2D(filters=12, kernel_size=(4,4), activation='relu')(X)\n",
    "        X = MaxPooling2D()(X)\n",
    "        X = Flatten()(X)\n",
    "        X = Dense(216, activation='relu')(X)\n",
    "        output = Dense(self.action_space, activation=\"tanh\")(X)\n",
    "\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss_continuous, optimizer=optimizer(lr=lr))\n",
    "        #print(self.Actor.summary())\n",
    "\n",
    "    def ppo_loss_continuous(self, y_true, y_pred):\n",
    "        advantages, actions, logp_old_ph, = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space]\n",
    "        LOSS_CLIPPING = 0.2\n",
    "        logp = self.gaussian_likelihood(actions, y_pred)\n",
    "\n",
    "        ratio = K.exp(logp - logp_old_ph)\n",
    "\n",
    "        p1 = ratio * advantages\n",
    "        p2 = tf.where(advantages > 0, (1.0 + LOSS_CLIPPING)*advantages, (1.0 - LOSS_CLIPPING)*advantages) # minimum advantage\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        return actor_loss\n",
    "\n",
    "    def gaussian_likelihood(self, actions, pred): # for keras custom loss\n",
    "        log_std = -0.5 * np.ones(self.action_space, dtype=np.float32)\n",
    "        pre_sum = -0.5 * (((actions-pred)/(K.exp(log_std)+1e-8))**2 + 2*log_std + K.log(2*np.pi))\n",
    "        return K.sum(pre_sum, axis=1)\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Actor.predict(state)\n",
    "\n",
    "\n",
    "class Critic_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        old_values = Input(shape=(1,))\n",
    "\n",
    "        X = Conv2D(filters=6, kernel_size=(7,7), strides=3, activation='relu')(X_input)\n",
    "        X = MaxPooling2D()(X)\n",
    "        X = Conv2D(filters=12, kernel_size=(4,4), activation='relu')(X)\n",
    "        X = MaxPooling2D()(X)\n",
    "        X = Flatten()(X)\n",
    "        X = Dense(216, activation='relu')(X)\n",
    "        value = Dense(1, activation=None)(X)\n",
    "\n",
    "        self.Critic = Model(inputs=[X_input, old_values], outputs = value)\n",
    "        self.Critic.compile(loss=[self.critic_PPO2_loss(old_values)], optimizer=optimizer(lr=lr))\n",
    "\n",
    "    def critic_PPO2_loss(self, values):\n",
    "        def loss(y_true, y_pred):\n",
    "            LOSS_CLIPPING = 0.2\n",
    "            clipped_value_loss = values + K.clip(y_pred - values, -LOSS_CLIPPING, LOSS_CLIPPING)\n",
    "            v_loss1 = (y_true - clipped_value_loss) ** 2\n",
    "            v_loss2 = (y_true - y_pred) ** 2\n",
    "            \n",
    "            value_loss = 0.5 * K.mean(K.maximum(v_loss1, v_loss2))\n",
    "            #value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "            return value_loss\n",
    "        return loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n",
    "        # return self.Critic.predict(state)\n",
    "    \n",
    "\n",
    "class PPOAgent:\n",
    "    # PPO Main Optimization Algorithm\n",
    "    def __init__(self, env_name, model_name=\"\"):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.shape[0]\n",
    "        self.state_size = self.env.observation_space.shape\n",
    "        self.EPISODES = 10000 # total episodes to train through all environments\n",
    "        self.episode = 0 # used to track the episodes total count of episodes played through all thread environments\n",
    "        self.max_average = 0 # when average score is above 0 model will be saved\n",
    "        self.lr = 0.00025\n",
    "        self.epochs = 10 # training epochs\n",
    "        self.shuffle = True\n",
    "        self.Training_batch = 5000\n",
    "        #self.optimizer = RMSprop\n",
    "        self.optimizer = Adam\n",
    "\n",
    "        self.replay_count = 0\n",
    "        self.writer = SummaryWriter(comment=\"_\"+self.env_name+\"_\"+self.optimizer.__name__+\"_\"+str(self.lr))\n",
    "        \n",
    "        # Instantiate plot memory\n",
    "        self.scores_, self.episodes_, self.average_ = [], [], [] # used in matplotlib plots\n",
    "\n",
    "        # Create Actor-Critic network models\n",
    "        self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        \n",
    "        self.Actor_name = f\"{self.env_name}_PPO_Actor.h5\"\n",
    "        self.Critic_name = f\"{self.env_name}_PPO_Critic.h5\"\n",
    "        #self.load() # uncomment to continue training from old weights\n",
    "\n",
    "        # do not change bellow\n",
    "        self.log_std = -0.5 * np.ones(self.action_size, dtype=np.float32)\n",
    "        self.std = np.exp(self.log_std)\n",
    "        self.std_new = 0.3\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next actions mu and sigma \n",
    "        pred = self.Actor.predict(np.expand_dims(state, axis=0))\n",
    "        pred_flat = pred.flatten()\n",
    "\n",
    "        # split the actions\n",
    "        steering = pred_flat[0]\n",
    "        gas = pred_flat[1]\n",
    "        breaks = pred_flat[2]\n",
    "\n",
    "        # print(f'PredFalt: {pred_flat}   Steering: {steering} Gas: {gas} Breaks: {breaks}')\n",
    "\n",
    "\n",
    "        gas_normalized = self.normalizeNP(gas)\n",
    "        breaks_normalized = self.normalizeNP(breaks)\n",
    "    \n",
    "\n",
    "        sampled_gas = np.random.normal(gas_normalized, self.std_new)\n",
    "\n",
    "        sampled_breaks = np.random.normal(breaks_normalized, self.std_new)\n",
    "\n",
    "        sampled_steering = np.random.normal(steering, self.std_new)\n",
    "\n",
    "\n",
    "        # clipping the values on bounds \n",
    "        clipped_gas = np.clip(sampled_gas, 0,1)\n",
    "        clipped_breaks =  np.clip(sampled_breaks, 0,1)\n",
    "        clipped_steering = np.clip(sampled_steering,-1,1)\n",
    "\n",
    "        # # concat the values back together to create one action tensor to sample from\n",
    "        action = np.array([clipped_steering,clipped_gas, clipped_breaks])\n",
    "\n",
    "        action = np.reshape(action, pred.shape)\n",
    "\n",
    "        logp_t = self.gaussian_likelihood(action, pred, self.log_std)\n",
    "\n",
    "        return action, logp_t\n",
    "\n",
    "\n",
    "    def normalize(self, tensor):\n",
    "        \n",
    "        #  set upper and lower bounds for the tanh produced outcome\n",
    "        min_bound = tf.constant([-1], dtype=tf.float32)\n",
    "        max_bound = tf.constant([1], dtype=tf.float32)\n",
    "        \n",
    "        #  create an epsilon to avoid division by 0\n",
    "        eps = tf.constant([1e-12], dtype=tf.float32)\n",
    "\n",
    "        # normalize by the following formular https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/\n",
    "        tensor = tf.divide((tensor - min_bound), tf.maximum((max_bound - min_bound),eps))\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def normalizeNP(self, x):\n",
    "        return ((x-(-1)) / np.maximum((1-(-1)), 1e-12))\n",
    "\n",
    "    def gaussian_likelihood(self, action, pred, log_std):\n",
    "        # https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/sac/policies.py\n",
    "        pre_sum = -0.5 * (((action-pred)/(np.exp(log_std)+1e-8))**2 + 2*log_std + np.log(2*np.pi)) \n",
    "        return np.sum(pre_sum, axis=1)\n",
    "\n",
    "    def discount_rewards(self, reward):#gaes is better\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        # We apply the discount and normalize it to avoid big variability of rewards\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.90, normalize=True):\n",
    "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "        deltas = np.stack(deltas)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(deltas) - 1)):\n",
    "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "        target = gaes + values\n",
    "        if normalize:\n",
    "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "        return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "    def replay(self, states, actions, rewards, dones, next_states, logp_ts):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        actions = np.vstack(actions)\n",
    "        logp_ts = np.vstack(logp_ts)\n",
    "\n",
    "        # Get Critic network predictions \n",
    "        # print(states.shape)\n",
    "        # print(np.expand_dims(states, axis=0).shape)\n",
    "        values = self.Critic.predict(states)\n",
    "\n",
    "        next_values = self.Critic.predict(next_states)\n",
    "\n",
    "        # Compute discounted rewards and advantages\n",
    "        #discounted_r = self.discount_rewards(rewards)\n",
    "        #advantages = np.vstack(discounted_r - values)\n",
    "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "        '''\n",
    "        pylab.plot(adv,'.')\n",
    "        pylab.plot(target,'-')\n",
    "        ax=pylab.gca()\n",
    "        ax.grid(True)\n",
    "        pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "        pylab.show()\n",
    "        if str(episode)[-2:] == \"00\": pylab.savefig(self.env_name+\"_\"+self.episode+\".png\")\n",
    "        '''\n",
    "        # stack everything to numpy array\n",
    "        # pack all advantages, predictions and actions to y_true and when they are received\n",
    "        # in custom loss function we unpack it\n",
    "        y_true = np.hstack([advantages, actions, logp_ts])\n",
    "        \n",
    "        # training Actor and Critic networks\n",
    "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "        c_loss = self.Critic.Critic.fit([states, values], target, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "\n",
    "        # calculate loss parameters (should be done in loss, but couldn't find working way how to do that with disabled eager execution)\n",
    "        pred = self.Actor.predict(states)\n",
    "        log_std = -0.5 * np.ones(self.action_size, dtype=np.float32)\n",
    "        logp = self.gaussian_likelihood(actions, pred, log_std)\n",
    "        approx_kl = np.mean(logp_ts - logp)\n",
    "        approx_ent = np.mean(-logp)\n",
    "\n",
    "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "        self.writer.add_scalar('Data/approx_kl_per_replay', approx_kl, self.replay_count)\n",
    "        self.writer.add_scalar('Data/approx_ent_per_replay', approx_ent, self.replay_count)\n",
    "        self.replay_count += 1\n",
    " \n",
    "    def load(self):\n",
    "        self.Actor.Actor.load_weights(self.Actor_name)\n",
    "        self.Critic.Critic.load_weights(self.Critic_name)\n",
    "\n",
    "    def save(self):\n",
    "        self.Actor.Actor.save_weights(self.Actor_name)\n",
    "        self.Critic.Critic.save_weights(self.Critic_name)\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "    def PlotModel(self, score, episode, save=True):\n",
    "        self.scores_.append(score)\n",
    "        self.episodes_.append(episode)\n",
    "        self.average_.append(sum(self.scores_[-50:]) / len(self.scores_[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes_, self.scores_, 'b')\n",
    "            pylab.plot(self.episodes_, self.average_, 'r')\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.grid(True)\n",
    "                pylab.savefig(self.env_name+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "        # saving best models\n",
    "        if self.average_[-1] >= self.max_average and save:\n",
    "            self.max_average = self.average_[-1]\n",
    "            self.save()\n",
    "            SAVING = \"SAVING\"\n",
    "            # decreaate learning rate every saved model\n",
    "            #self.lr *= 0.99\n",
    "            #K.set_value(self.Actor.Actor.optimizer.learning_rate, self.lr)\n",
    "            #K.set_value(self.Critic.Critic.optimizer.learning_rate, self.lr)\n",
    "        else:\n",
    "            SAVING = \"\"\n",
    "\n",
    "        return self.average_[-1], SAVING\n",
    "    \n",
    "    def run_batch(self):\n",
    "        state = self.env.reset()\n",
    "        #print(state.shape)\n",
    "        done, score, SAVING = False, 0, ''\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            states, next_states, actions, rewards, dones, logp_ts = [], [], [], [], [], []\n",
    "            for t in range(self.Training_batch):\n",
    "                # self.env.render()\n",
    "                if self.episode % 1000:\n",
    "                    self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, logp_t = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action[0])\n",
    "                # Memorize (state, next_states, action, reward, done, logp_ts) for training\n",
    "                states.append(np.expand_dims(state, axis=0))\n",
    "                # next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
    "                next_states.append(np.expand_dims(next_state, axis=0))\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                logp_ts.append(logp_t[0])\n",
    "\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average, SAVING = self.PlotModel(score, self.episode)\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/average_score',  average, self.episode)\n",
    "                    \n",
    "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n",
    "                    #state = np.reshape(state, [1, self.state_size[0]])\n",
    "\n",
    "            self.replay(states, actions, rewards, dones, next_states, logp_ts)\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "    def run_multiprocesses(self, num_worker = 4):\n",
    "        works, parent_conns, child_conns = [], [], []\n",
    "        for idx in range(num_worker):\n",
    "            parent_conn, child_conn = Pipe()\n",
    "            work = Environment(idx, child_conn, self.env_name, self.state_size[0], self.action_size, True)\n",
    "            work.start()\n",
    "            works.append(work)\n",
    "            parent_conns.append(parent_conn)\n",
    "            child_conns.append(child_conn)\n",
    "\n",
    "        states =        [[] for _ in range(num_worker)]\n",
    "        next_states =   [[] for _ in range(num_worker)]\n",
    "        actions =       [[] for _ in range(num_worker)]\n",
    "        rewards =       [[] for _ in range(num_worker)]\n",
    "        dones =         [[] for _ in range(num_worker)]\n",
    "        logp_ts =       [[] for _ in range(num_worker)]\n",
    "        score =         [0 for _ in range(num_worker)]\n",
    "\n",
    "        state = [0 for _ in range(num_worker)]\n",
    "        for worker_id, parent_conn in enumerate(parent_conns):\n",
    "            state[worker_id] = parent_conn.recv()\n",
    "\n",
    "        while self.episode < self.EPISODES:\n",
    "            # get batch of action's and log_pi's\n",
    "            action, logp_pi = self.act(np.reshape(state, [num_worker, self.state_size[0]]))\n",
    "            \n",
    "            for worker_id, parent_conn in enumerate(parent_conns):\n",
    "                parent_conn.send(action[worker_id])\n",
    "                actions[worker_id].append(action[worker_id])\n",
    "                logp_ts[worker_id].append(logp_pi[worker_id])\n",
    "\n",
    "            for worker_id, parent_conn in enumerate(parent_conns):\n",
    "                next_state, reward, done, _ = parent_conn.recv()\n",
    "\n",
    "                states[worker_id].append(state[worker_id])\n",
    "                next_states[worker_id].append(next_state)\n",
    "                rewards[worker_id].append(reward)\n",
    "                dones[worker_id].append(done)\n",
    "                state[worker_id] = next_state\n",
    "                score[worker_id] += reward\n",
    "\n",
    "                if done:\n",
    "                    average, SAVING = self.PlotModel(score[worker_id], self.episode)\n",
    "                    print(\"episode: {}/{}, worker: {}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, worker_id, score[worker_id], average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{num_worker}/score_per_episode', score[worker_id], self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{num_worker}/learning_rate', self.lr, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{num_worker}/average_score',  average, self.episode)\n",
    "                    score[worker_id] = 0\n",
    "                    if(self.episode < self.EPISODES):\n",
    "                        self.episode += 1\n",
    "                        \n",
    "                        \n",
    "            for worker_id in range(num_worker):\n",
    "                if len(states[worker_id]) >= self.Training_batch:\n",
    "                    self.replay(states[worker_id], actions[worker_id], rewards[worker_id], dones[worker_id], next_states[worker_id], logp_ts[worker_id])\n",
    "\n",
    "                    states[worker_id] = []\n",
    "                    next_states[worker_id] = []\n",
    "                    actions[worker_id] = []\n",
    "                    rewards[worker_id] = []\n",
    "                    dones[worker_id] = []\n",
    "                    logp_ts[worker_id] = []\n",
    "\n",
    "        # terminating processes after a while loop\n",
    "        works.append(work)\n",
    "        for work in works:\n",
    "            work.terminate()\n",
    "            print('TERMINATED:', work)\n",
    "            work.join()\n",
    "\n",
    "    def test(self, test_episodes = 100):#evaluate\n",
    "        self.load()\n",
    "        for e in range(101):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size[0]])\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.Actor.predict(state)[0]\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    average, SAVING = self.PlotModel(score, e, save=False)\n",
    "                    print(\"episode: {}/{}, score: {}, average{}\".format(e, test_episodes, score, average))\n",
    "                    break\n",
    "        self.env.close()\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env_name = 'CarRacing-v0'\n",
    "    agent = PPOAgent(env_name)\n",
    "    agent.run_batch() # train as PPO\n",
    " \n",
    "    #agent.run_multiprocesses(num_worker = 16)  # train PPO multiprocessed (fastest)\n",
    "    #agent.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ann')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "\n",
    "    def __init__(self, shape=(14,14), num_negative_tiles=0, starting_point=(1,1),num_walls=10):\n",
    "\n",
    "\n",
    "        self.size = shape\n",
    "        self.num_negative_tiles = num_negative_tiles\n",
    "        self.current_state = np.array(starting_point)\n",
    "        self.grid=None\n",
    "        self.stochastic_transistions = np.zeros(shape=(shape[0]-1,shape[1]-1), dtype=np.float32)\n",
    "        self.goal = np.array((shape[0]-2, shape[1]-2))\n",
    "        self.acc_reward = 0 \n",
    "        self.generate_world(num_walls)\n",
    "        \n",
    "    def generate_world(self, num_walls):\n",
    "        valid_grid = False\n",
    "        while not valid_grid:\n",
    "            self.grid = self.generate_grid()\n",
    "            unique_wall_indexes = False\n",
    "            while not unique_wall_indexes:\n",
    "                random_walls = np.random.randint(1,self.size[0]-1,size=(num_walls,2))\n",
    "                random_walls_unique = np.unique(random_walls,axis=0)\n",
    "                # check that the indeces neither describe the starting point nor the goal state or are doubled\n",
    "                if len(random_walls) == len(random_walls_unique) and not tuple(self.goal) in random_walls.tolist() and not tuple(self.current_state) in random_walls.tolist():\n",
    "                    unique_wall_indexes = True\n",
    "            # apply for all unique indexes \n",
    "            for r in random_walls:\n",
    "                self.grid[r[0],r[1]] = -100\n",
    "            valid_grid = self.grid_is_valid()\n",
    "    \n",
    "    def generate_grid(self):\n",
    "        grid = np.zeros(self.size, dtype=np.int64)\n",
    "        grid[self.goal[0]][self.goal[1]] = 100\n",
    "        grid[0,:] = -100\n",
    "        grid[self.size[1]-1,:] = -100\n",
    "        grid[:,0] = -100\n",
    "        grid[:,self.size[1]-1] = -100\n",
    "        return grid\n",
    "\n",
    "          \n",
    "    # depth first search to find out if the generated grid world is valid i.e there is a path from start to goal\n",
    "    def grid_is_valid(self):\n",
    "        visited = []\n",
    "        visited = self.dfs(visited, self.current_state)     \n",
    "        return np.any(np.all(self.goal == visited, axis=1))\n",
    "\n",
    "    def dfs(self, visited, node):\n",
    "        visited.append(node)\n",
    "        neighbors = self.get_neighbors(node)\n",
    "        for neighbor in neighbors:\n",
    "            if not np.any(np.all(neighbor == np.array(visited), axis=1)):\n",
    "            # neighbor is not in visited\n",
    "                if self.grid[neighbor[0]][neighbor[1]] != -100:\n",
    "                # neighbor is not a wall\n",
    "                    self.dfs(visited, neighbor)\n",
    "        return visited\n",
    "            \n",
    "    def get_neighbors(self, node):\n",
    "        # edges in the same order as the step function, important !!!\n",
    "        edges = [(0,1), (0,-1), (-1,0), (1,0)]\n",
    "        neighbors = []\n",
    "        for edge in edges:\n",
    "            neighbor = (node[0] + edge[0], node[1] + edge[1])\n",
    "            neighbors.append(np.array(neighbor))\n",
    "        return neighbors\n",
    "        \n",
    "        \n",
    "    # reset the actor to starting state\n",
    "    def reset(self):\n",
    "        self.current_state = (1,1)\n",
    "        self.acc_reward = 0\n",
    "\n",
    "\n",
    "    def step(self,action):\n",
    "        '''\n",
    "        Args:\n",
    "        action(): 0: right, 1: left, 2, up, 3, down\n",
    "        throws error if move is invalid due to wall\n",
    "        '''\n",
    "        \n",
    "        # anders ugly mit if elif statements, switch erst ab python 3.10\n",
    "        # right\n",
    "        if action == 0:\n",
    "            # get current state \n",
    "            y, x = self.current_state\n",
    "            # check that current state is accessable\n",
    "            new_y,new_x = y, x+1\n",
    "            if self.grid[new_y,new_x] != -100:\n",
    "                # update current state and collect rewward\n",
    "                self.current_state = ((new_y, new_x))\n",
    "                self.acc_reward += self.grid[new_y,new_x]\n",
    "            else:\n",
    "                raise ValueError('Could not move there due to wall.')\n",
    "\n",
    "\n",
    "        # left step\n",
    "        elif action == 1:\n",
    "            # get current state \n",
    "            y, x = self.current_state\n",
    "            # check that current state is accessable\n",
    "            new_y,new_x = y, x-1\n",
    "            if self.grid[new_y,new_x] != -100:\n",
    "                # update current state and collect rewward\n",
    "                self.current_state = ((new_y, new_x))\n",
    "                self.acc_reward += self.grid[new_y,new_x]\n",
    "            else:\n",
    "                raise ValueError('Could not move there due to wall.')\n",
    "\n",
    "        # upwards step\n",
    "        elif action == 2:\n",
    "            # get current state \n",
    "            y, x = self.current_state\n",
    "            # check that current state is accessable\n",
    "            new_y,new_x = y-1, x\n",
    "            if self.grid[new_y,new_x] != -100:\n",
    "                # update current state and collect rewward\n",
    "                self.current_state = ((new_y, new_x))\n",
    "                self.acc_reward += self.grid[new_y,new_x]\n",
    "            else:\n",
    "                raise ValueError('Could not move there due to wall.')\n",
    "        \n",
    "        # downwards step\n",
    "        elif action == 3:\n",
    "            # get current state \n",
    "            y, x = self.current_state\n",
    "            # check that current state is accessable\n",
    "            new_y,new_x = y+1, x\n",
    "            if self.grid[new_y,new_x] != -100:\n",
    "                # update current state and collect rewward\n",
    "                self.current_state = ((new_y, new_x))\n",
    "                self.acc_reward += self.grid[new_y,new_x]\n",
    "            else:\n",
    "                raise ValueError('Could not move there due to wall.')\n",
    "        else:\n",
    "            raise ValueError('Action index out of bounds. Actions-space = (0,1,2,3)')\n",
    "    \n",
    "        \n",
    "    # print the gridworld as an array \n",
    "    def visualize(self):\n",
    "        print(self.grid)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsAgent:\n",
    "    def __init__(self, grid_world, state, epsilon=0.9, alpha=0.5, gamma=0.05):\n",
    "        self.learning_rate = alpha\n",
    "        self.discount_factor = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.current_state = state\n",
    "        self.grid_world = grid_world\n",
    "        #self.size = tuple(np.append(np.subtract(grid_world.size, (2,2)), np.array(4)))\n",
    "        self.size = tuple(np.append(np.array(grid_world.size), np.array(4)))\n",
    "        self.q_table = np.zeros((self.size), dtype=np.float32)\n",
    "        \n",
    "    def get_reward(self,state):\n",
    "        return self.grid_world.grid[state[0]][state[1]]\n",
    "    \n",
    "    def get_valid_actions(self, state):\n",
    "        # get a list of all actions we can do for the next step\n",
    "        actions = []\n",
    "        grid = self.grid_world\n",
    "        neighbors = grid.get_neighbors(state)\n",
    "        for idx, neighbor in enumerate(neighbors):\n",
    "            # if neighbor is not a wall, add action to move there as valid\n",
    "            if grid.grid[neighbor[0]][neighbor[1]] != -100:\n",
    "                actions.append(idx)\n",
    "        return actions\n",
    "      \n",
    "    def choose_action(self, state):\n",
    "        # choose the next action for the agent\n",
    "        \n",
    "        actions = self.get_valid_actions(state)\n",
    "        #print(f'actions: {actions}')\n",
    "        \n",
    "        # choose random action\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(actions)\n",
    "        # choose highest q-value action\n",
    "        else:\n",
    "            valid_states = {}\n",
    "            for i in actions:\n",
    "                valid_states[i] = (self.q_table[state][i])\n",
    "                \n",
    "            #print(f'valid_states:{valid_states}')\n",
    "            \n",
    "            # argmax for dictionary\n",
    "            action = max(valid_states, key=valid_states.get)\n",
    "            \n",
    "        #print(f'chosen action: {action}')\n",
    "        return action\n",
    "            \n",
    "    def learn(self, n_steps):   \n",
    "        # implement Sarasa (with n_steps)\n",
    "        self.current_state = self.grid_world.current_state\n",
    "        state = self.current_state\n",
    "        next_action = None\n",
    "        rewards = 0\n",
    "        for n in range(n_steps):\n",
    "            # get discounted rewards \n",
    "            reward = self.get_reward(state) * (self.discount_factor** n)\n",
    "            rewards += reward\n",
    "            action = self.choose_action(state)\n",
    "            if next_action == None:\n",
    "            # save the first action to move \n",
    "                next_action = action\n",
    "            \n",
    "            # get the next state for n-step Sarsa\n",
    "            state = self.do_fake_step(state, action)\n",
    "        Q = self.q_table[tuple(self.current_state)][next_action] \n",
    "        final_q = self.q_table[state][action] * (self.discount_factor** n_steps)\n",
    "        #print(f'final_q: {final_q}\\n rewards: {rewards}\\n Q: {Q}')\n",
    "        Q = rewards + final_q - Q\n",
    "        #print(f'Q:{Q}')\n",
    "        self.q_table[tuple(self.current_state)][action] = self.q_table[tuple(self.current_state)][action] + (self.learning_rate * Q)\n",
    "        #print(f'next_action: {next_action}')\n",
    "        return next_action\n",
    "        \n",
    "        \n",
    "    def do_fake_step(self, state, action):\n",
    "        # get the next state if we move according to the given action\n",
    "        grid = self.grid_world.grid     \n",
    "        edges = [(0,1), (0,-1), (-1,0), (1,0)]\n",
    "        state = (state[0] + edges[action][0], state[1] + edges[action][1])\n",
    "        return state\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[[-100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100]\n",
      " [-100    0    0    0    0    0    0    0    0    0    0    0    0 -100]\n",
      " [-100    0    0    0    0    0    0    0    0    0    0    0    0 -100]\n",
      " [-100    0    0    0    0    0    0    0 -100 -100 -100    0    0 -100]\n",
      " [-100    0    0    0    0 -100    0    0 -100    0    0    0    0 -100]\n",
      " [-100    0    0 -100    0    0    0    0    0    0    0    0    0 -100]\n",
      " [-100    0    0    0    0    0    0    0    0    0    0    0    0 -100]\n",
      " [-100 -100    0    0    0    0 -100    0    0    0    0    0 -100 -100]\n",
      " [-100    0    0    0    0    0    0    0    0    0    0    0    0 -100]\n",
      " [-100    0    0    0    0    0    0    0    0    0    0    0    0 -100]\n",
      " [-100    0    0    0    0    0    0 -100    0    0    0    0    0 -100]\n",
      " [-100    0    0    0    0    0    0    0    0    0    0    0    0 -100]\n",
      " [-100    0    0    0    0    0    0    0    0    0    0    0  100 -100]\n",
      " [-100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100]]\n"
     ]
    }
   ],
   "source": [
    "grid = Gridworld()\n",
    "#grid.step(0)\n",
    "valid_states = grid.grid_is_valid()\n",
    "print(valid_states)\n",
    "grid.visualize()\n",
    "#print(grid.stochastic_transistions)\n",
    "agent = SarsAgent(grid, grid.current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 101/3000 [00:06<03:14, 14.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_25639/3510303985.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(grid.current_state, tuple(grid.goal))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jk/h_92czjx5jjcf631wf82ht9c0000gn/T/ipykernel_25639/109439299.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, n_steps)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# get discounted rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[0;34m**\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 3000\n",
    "for episode in tqdm(range(episodes)):\n",
    "    grid.reset()\n",
    "    #print(grid.current_state, tuple(grid.goal))\n",
    "    while grid.current_state != tuple(grid.goal):\n",
    "        action = agent.learn(3)\n",
    "        \n",
    "        grid.step(action)\n",
    "        #print(grid.current_state)\n",
    "    if agent.epsilon > 0.05:\n",
    "        agent.epsilon = agent.epsilon * 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 30.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6170, 2150, 1850, 1376, 1558, 3486, 3038, 4346, 8298, 15386, 3832, 1698, 11666, 8410, 15246, 3512, 2614, 11200, 1522, 2678, 1346, 11046, 9176, 3330, 2308, 1160, 1938, 5986, 3706, 2786, 1144, 6658, 1058, 38102, 6510, 10094, 1540, 7330, 1170, 796, 3838, 3864, 4372, 3304, 2184, 2710, 998, 6388, 1438, 3090, 3218, 2758, 3662, 5598, 2704, 686, 4602, 1132, 1246, 3378, 4032, 2200, 2578, 4922, 2320, 9500, 10878, 6546, 2398, 586, 14904, 10788, 4062, 20140, 5858, 4602, 1464, 1562, 2344, 1964, 1612, 6088, 1378, 4406, 654, 2558, 2572, 3168, 7404, 1660, 5028, 2870, 580, 1416, 12594, 3124, 3012, 7286, 3232, 3144]\n",
      "4758.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grid.reset()\n",
    "num_steps = []\n",
    "for r in tqdm(range(100)):\n",
    "    grid.reset()\n",
    "    i = 0\n",
    "    while grid.current_state != tuple(grid.goal):\n",
    "        grid.step(agent.choose_action(grid.current_state))\n",
    "        i += 1\n",
    "    num_steps.append(i)\n",
    "    \n",
    "\n",
    "print(num_steps)\n",
    "print(str(sum(num_steps)/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

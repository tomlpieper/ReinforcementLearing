{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "\n",
    "    def __init__(self, shape=(10,10), num_negative_tiles=0, starting_point=(1,1),goal=(8,8),num_walls=20):\n",
    "\n",
    "\n",
    "        self.size = shape\n",
    "        self.num_negative_tiles = num_negative_tiles\n",
    "        self.current_state = np.array(starting_point)\n",
    "        self.grid = np.zeros(shape, dtype=np.int64)\n",
    "        self.stochastic_transistions = np.zeros(shape=(shape[0]-1,shape[1]-1), dtype=np.float32)\n",
    "        self.goal = np.array(goal)\n",
    "        self.acc_reward = 0 \n",
    "\n",
    "        # set values for entries in gridworld (rewards, goal, penalties) non-doable tiles should yield -100\n",
    "        self.grid[goal[0]][goal[1]] = 100\n",
    "\n",
    "\n",
    "        # setting borders\n",
    "        self.grid[0,:] = -100\n",
    "        self.grid[shape[1]-1,:] = -100\n",
    "        self.grid[:,0] = -100\n",
    "        self.grid[:,shape[1]-1] = -100\n",
    "\n",
    "\n",
    "\n",
    "        # implementing random negative tiles \n",
    "\n",
    "        \n",
    "        # check if indexes are unique\n",
    "        unique_wall_indexes = False\n",
    "        valid_grid = False\n",
    "        \n",
    "        while not unique_wall_indexes:\n",
    "            random_walls = np.random.randint(1,shape[0]-1,size=(num_walls,2))\n",
    "            random_walls_unique = np.unique(random_walls,axis=0)\n",
    "            # check that the indeces neither describe the starting point nor the goal state or are doubled\n",
    "            if len(random_walls) == len(random_walls_unique) and not goal in random_walls.tolist() and not starting_point in random_walls.tolist():\n",
    "                unique_wall_indexes = True\n",
    "        # apply for all unique indexes \n",
    "        for r in random_walls:\n",
    "            self.grid[r[0],r[1]] = -100\n",
    "        \n",
    "        valid_grid = self.grid_is_valid()\n",
    "        \n",
    "          \n",
    "    # depth first search to find out if the generated grid world is valid i.e there is a path from start to goal\n",
    "    def grid_is_valid(self):\n",
    "        visited = []\n",
    "        visited = self.dfs(visited, self.current_state)     \n",
    "        return np.any(np.all(self.goal == visited, axis=1))\n",
    "\n",
    "    def dfs(self, visited, node):\n",
    "        visited.append(node)\n",
    "        neighbors = self.get_neighbors(node)\n",
    "        for neighbor in neighbors:\n",
    "            if not np.any(np.all(neighbor == np.array(visited), axis=1)):\n",
    "            # neighbor is not in visited\n",
    "                if self.grid[neighbor[0]][neighbor[1]] != -100:\n",
    "                # neighbor is not a wall\n",
    "                    self.dfs(visited, neighbor)\n",
    "        return visited\n",
    "            \n",
    "    def get_neighbors(self, node):\n",
    "        edges = [(1,0), (0,1), (-1,0), (0,-1)]\n",
    "        neighbors = []\n",
    "        for edge in edges:\n",
    "            neighbor = (node[0] + edge[0], node[1] + edge[1])\n",
    "            neighbors.append(np.array(neighbor))\n",
    "        return neighbors\n",
    "        \n",
    "        \n",
    "    # reset the actor to starting state\n",
    "    def reset(self):\n",
    "        self.current_state = (1,1)\n",
    "        self.acc_reward = 0\n",
    "\n",
    "\n",
    "\n",
    "    def step(self,action):\n",
    "        '''\n",
    "        Args:\n",
    "        action(): 0: right, 1: left, 2, up, 3, down\n",
    "        throws error if move is invalid due to wall\n",
    "        '''\n",
    "        \n",
    "        # anders ugly mit if elif statements, switch erst ab python 3.10\n",
    "        # right\n",
    "        if action == 0:\n",
    "            # get current state \n",
    "            y, x = self.current_state\n",
    "            # check that current state is accessable\n",
    "            new_y,new_x = y, x+1\n",
    "            if self.grid[new_y,new_x] != -100:\n",
    "                # update current state and collect rewward\n",
    "                self.current_state = ((new_y, new_x))\n",
    "                self.acc_reward += self.grid[new_y,new_x]\n",
    "            else:\n",
    "                raise ValueError('Could not move there due to wall.')\n",
    "\n",
    "\n",
    "        # left step\n",
    "        elif action == 1:\n",
    "            # get current state \n",
    "            y, x = self.current_state\n",
    "            # check that current state is accessable\n",
    "            new_y,new_x = y, x-1\n",
    "            if self.grid[new_y,new_x] != -100:\n",
    "                # update current state and collect rewward\n",
    "                self.current_state = ((new_y, new_x))\n",
    "                self.acc_reward += self.grid[new_y,new_x]\n",
    "            else:\n",
    "                raise ValueError('Could not move there due to wall.')\n",
    "\n",
    "        # upwards step\n",
    "        elif action == 2:\n",
    "            # get current state \n",
    "            y, x = self.current_state\n",
    "            # check that current state is accessable\n",
    "            new_y,new_x = y-1, x\n",
    "            if self.grid[new_y,new_x] != -100:\n",
    "                # update current state and collect rewward\n",
    "                self.current_state = ((new_y, new_x))\n",
    "                self.acc_reward += self.grid[new_y,new_x]\n",
    "            else:\n",
    "                raise ValueError('Could not move there due to wall.')\n",
    "        \n",
    "        # downwards step\n",
    "        elif action == 3:\n",
    "            # get current state \n",
    "            y, x = self.current_state\n",
    "            # check that current state is accessable\n",
    "            new_y,new_x = y+1, x\n",
    "            if self.grid[new_y,new_x] != -100:\n",
    "                # update current state and collect rewward\n",
    "                self.current_state = ((new_y, new_x))\n",
    "                self.acc_reward += self.grid[new_y,new_x]\n",
    "            else:\n",
    "                raise ValueError('Could not move there due to wall.')\n",
    "        else:\n",
    "            raise ValueError('Action index out of bounds. Actions-space = (0,1,2,3)')\n",
    "    \n",
    "        \n",
    "    # print the gridworld as an array \n",
    "    def visualize(self):\n",
    "        print(self.grid)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsAgent:\n",
    "    def __init__(self, grid_world, state, epsilon=0.9, alpha=0.5, gamma=0.95):\n",
    "        self.learning_rate = alpha\n",
    "        self.discount_factor = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.current_state = state\n",
    "        self.grid_world = grid_world\n",
    "        self.size = tuple(np.append(np.subtract(grid_world.size, (2,2)), np.array(4)))\n",
    "        print(self.size)\n",
    "        self.q_table = np.zeros((self.size), dtype=np.float32)\n",
    "        \n",
    "    def get_reward(self,state):\n",
    "        return self.grid_world.grid[state[0]][state[1]]\n",
    "    \n",
    "    def get_valid_actions(self, state):\n",
    "        actions = []\n",
    "        grid = self.grid_world\n",
    "        neighbors = grid.get_neighbors(state)\n",
    "        for idx, neighbor in enumerate(neighbors):\n",
    "            if grid.grid[neighbor[0]][neighbor[1]] != -100:\n",
    "                actions.append(idx)\n",
    "        return actions\n",
    "      \n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        # get a list of all actions we can do for the next step\n",
    "        actions = self.get_valid_actions(state)\n",
    "        \n",
    "        # choose random action\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(actions)\n",
    "        # choose highest q-value action\n",
    "        else:\n",
    "            valid_states = []\n",
    "            for action in actions:\n",
    "                valid_states.append(q_table[state, action])     \n",
    "            action = np.argmax(valid_states)\n",
    "        return action\n",
    "            \n",
    "    def learn(self, n_steps):\n",
    "        self.q_table[self.current_state, action] = self.q_table[self.current_state, action] + self.learning_rate*((reward+self.discount_factor*self.q_table[next_state, next_action])-self.q_table[self.current_state,action])\n",
    "        Q = self.q_table[self.current_state, action]\n",
    "        \n",
    "        reward = self.get_reward(state)\n",
    "        \n",
    "        \n",
    "        target = reward + self.discount_factor**n * q_table(state, action)\n",
    "        \n",
    "        reward + self.discount_factor**n * \n",
    "        \n",
    "        state = self.current_state\n",
    "        for n in range(n_steps):\n",
    "            reward = self.get_reward(state) * (self.discount_factor** n)\n",
    "            rewards.append(reward)\n",
    "            action = choose_action(state)\n",
    "            state = do_fake_step(action)\n",
    "        final_q = self.q_table[state, action] * (self.discount_factor** n_steps)\n",
    "        Q = rewards + final_q - Q\n",
    "        \n",
    "    def fake_step(self, action):\n",
    "        \n",
    "        return state\n",
    "            \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-100 -100 -100 -100 -100 -100 -100 -100 -100 -100]\n",
      " [-100 -100    0 -100    0    0    0 -100    0 -100]\n",
      " [-100    0    0    0    0    0 -100 -100 -100 -100]\n",
      " [-100    0 -100 -100    0 -100    0 -100    0 -100]\n",
      " [-100    0 -100    0    0    0    0    0    0 -100]\n",
      " [-100 -100    0    0    0    0    0    0    0 -100]\n",
      " [-100 -100    0    0 -100    0 -100 -100 -100 -100]\n",
      " [-100    0    0    0    0    0 -100    0 -100 -100]\n",
      " [-100    0    0 -100    0    0    0    0  100 -100]\n",
      " [-100 -100 -100 -100 -100 -100 -100 -100 -100 -100]]\n",
      "(8, 8, 4)\n",
      "q_table:\n",
      " [[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "grid = Gridworld()\n",
    "#grid.step(0)\n",
    "valid_states = grid.grid_is_valid()\n",
    "grid.visualize()\n",
    "#print(grid.stochastic_transistions)\n",
    "agent = SarsAgent(grid)\n",
    "print(f'q_table:\\n {agent.q_table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple expected at most 1 arguments, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5n/lggt_xf57s78jjmq538wdr6w0000gn/T/ipykernel_3925/647750193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple expected at most 1 arguments, got 2"
     ]
    }
   ],
   "source": [
    "x = (2,3)\n",
    "y = np.array((2,3))\n",
    "x == y\n",
    "x = tuple(2,3)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ce7daced66c1b43e67ee1266804bcc56425fa4e39cc8300d2c0d41d8b5ef83"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
